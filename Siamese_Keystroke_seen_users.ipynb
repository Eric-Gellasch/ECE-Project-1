{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa3f10f"
      },
      "source": [
        "\n",
        "# Siamese Keystroke Authentication\n",
        "\n",
        "This notebook trains a **Siamese MLP** for **fixed‑text keystroke authentication** and evaluates it in a way that matches the **LightGBM paper's binary setup**:\n",
        "\n",
        "- **Seen users only** for now\n",
        "- **Per‑user binary task authentication**: *user u* vs *all others*\n",
        "- **70/30 split** of samples **per user**\n",
        "- Report **per‑user Precision/Recall/F1/FAR/FRR** and their **macro means**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a69a9b"
      },
      "source": [
        "## 1) Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "127a4146",
        "outputId": "43ee9072-362b-4f12-dfc4-334c72d25df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config set.\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"/content/fixed-text.csv\"\n",
        "SEED = 1337\n",
        "USE_CUDA = True\n",
        "\n",
        "TRAIN_FRACTION_PER_USER = 0.70\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 256\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "MAX_POS_PAIRS_PER_USER = 200\n",
        "NEGATIVES_PER_POSITIVE = 2\n",
        "\n",
        "EMBED_DIM = 128\n",
        "DROPOUT_P = 0.10\n",
        "MARGIN = 1.2\n",
        "\n",
        "print(\"Config set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fffd2efc"
      },
      "source": [
        "## 2) Imports & device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d7cef28",
        "outputId": "ab1fc463-3195-409e-f656-772627cc7f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, re, numpy as np, pandas as pd\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "DEVICE = \"cuda\" if (USE_CUDA and torch.cuda.is_available()) else \"cpu\"\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d60e83a"
      },
      "source": [
        "## 3) Load data → keep digraphs → 70/30 split → standardize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba7b1f51",
        "outputId": "b6cab91c-4a23-4bff-b943-c57686c53e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users: 99 | Train rows: 13839 | Test rows: 5933 | Features: 45\n"
          ]
        }
      ],
      "source": [
        "raw = pd.read_csv(DATA_PATH)\n",
        "digraph_cols = [c for c in raw.columns if re.match(r'^[DU][DU]\\.[^.]+\\.[^.]+$', c)]\n",
        "df = raw[['participant','repetition'] + digraph_cols].dropna(subset=digraph_cols).copy()\n",
        "df['participant'] = df['participant'].astype(str)\n",
        "\n",
        "# split by repetition per user\n",
        "train_parts, test_parts = [], []\n",
        "for uid, g in df.groupby('participant'):\n",
        "    g = g.sort_values('repetition')\n",
        "    k = int(round(TRAIN_FRACTION_PER_USER * len(g)))\n",
        "    train_parts.append(g.iloc[:k]); test_parts.append(g.iloc[k:])\n",
        "train_df = pd.concat(train_parts, ignore_index=True)\n",
        "test_df  = pd.concat(test_parts,  ignore_index=True)\n",
        "\n",
        "# scale on train, apply to test\n",
        "scaler = StandardScaler()\n",
        "train_df.loc[:, digraph_cols] = scaler.fit_transform(train_df[digraph_cols].astype('float32'))\n",
        "test_df.loc[:,  digraph_cols] = scaler.transform(   test_df[digraph_cols].astype('float32'))\n",
        "\n",
        "print(f\"Users: {df['participant'].nunique()} | Train rows: {len(train_df)} | Test rows: {len(test_df)} | Features: {len(digraph_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0ba2a1"
      },
      "source": [
        "## 4) Pair dataset (compact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2078afcc"
      },
      "outputs": [],
      "source": [
        "class PairDataset(Dataset):\n",
        "    \"\"\"Balanced positive/negative pairs for Siamese training (rebuilt each epoch).\"\"\"\n",
        "    def __init__(self, frame, feature_cols, max_pos_per_user=200, neg_per_pos=2, seed=SEED):\n",
        "        rng = np.random.RandomState(seed)\n",
        "        self.pairs = []\n",
        "        groups = {u: g[feature_cols].values.astype('float32') for u, g in frame.groupby('participant')}\n",
        "        users = list(groups.keys())\n",
        "\n",
        "        # positives\n",
        "        for u in users:\n",
        "            X = groups[u]; n = len(X)\n",
        "            if n < 2: continue\n",
        "            target = min(max_pos_per_user, n*(n-1)//2)\n",
        "            made, seen = 0, set()\n",
        "            while made < target:\n",
        "                i, j = rng.randint(0, n), rng.randint(0, n)\n",
        "                if i==j: continue\n",
        "                key = (i,j) if i<j else (j,i)\n",
        "                if key in seen: continue\n",
        "                seen.add(key); self.pairs.append((X[i], X[j], 1)); made += 1\n",
        "\n",
        "        # negatives\n",
        "        total_pos = len(self.pairs)\n",
        "        if len(users) >= 2:\n",
        "            for _ in range(total_pos * neg_per_pos):\n",
        "                ua, ub = rng.choice(users, size=2, replace=True)\n",
        "                while ub == ua and len(users) > 1:\n",
        "                    ub = rng.choice(users)\n",
        "                Xa, Xb = groups[ua], groups[ub]\n",
        "                ia, ib = rng.randint(0, len(Xa)), rng.randint(0, len(Xb))\n",
        "                self.pairs.append((Xa[ia], Xb[ib], 0))\n",
        "\n",
        "        rng.shuffle(self.pairs)\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "    def __getitem__(self, i):\n",
        "        xa, xb, y = self.pairs[i]\n",
        "        return torch.from_numpy(xa), torch.from_numpy(xb), torch.tensor([y], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "671db993"
      },
      "source": [
        "## 5) Siamese MLP (LayerNorm) + contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19955d20"
      },
      "outputs": [],
      "source": [
        "class SiameseMLP(nn.Module):\n",
        "    def __init__(self, d_in, d_emb=128, p_drop=0.10):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(d_in, 256), nn.ReLU(), nn.LayerNorm(256), nn.Dropout(p_drop),\n",
        "            nn.Linear(256, 128), nn.ReLU(), nn.LayerNorm(128),\n",
        "            nn.Linear(128, d_emb)\n",
        "        )\n",
        "    def embed(self, x):\n",
        "        z = self.f(x); return F.normalize(z, p=2, dim=1)\n",
        "    def forward(self, xa, xb): return self.embed(xa), self.embed(xb)\n",
        "\n",
        "def contrastive_loss(za, zb, y, margin=1.2):\n",
        "    d = torch.norm(za - zb, dim=1)\n",
        "    return (y.view(-1)*(d**2) + (1-y.view(-1))*F.relu(margin-d)**2).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45446f01"
      },
      "source": [
        "## 6) Train (fixed epochs, no validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95edf9c",
        "outputId": "da6c0a40-7509-4fd5-bb47-d89ce68ade52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01  loss=0.2152\n",
            "Epoch 02  loss=0.1925\n",
            "Epoch 03  loss=0.1861\n",
            "Epoch 04  loss=0.1831\n",
            "Epoch 05  loss=0.1774\n",
            "Epoch 06  loss=0.1748\n",
            "Epoch 07  loss=0.1715\n",
            "Epoch 08  loss=0.1680\n",
            "Epoch 09  loss=0.1652\n",
            "Epoch 10  loss=0.1634\n",
            "Epoch 11  loss=0.1602\n",
            "Epoch 12  loss=0.1580\n",
            "Epoch 13  loss=0.1568\n",
            "Epoch 14  loss=0.1563\n",
            "Epoch 15  loss=0.1532\n",
            "Epoch 16  loss=0.1527\n",
            "Epoch 17  loss=0.1499\n",
            "Epoch 18  loss=0.1485\n",
            "Epoch 19  loss=0.1472\n",
            "Epoch 20  loss=0.1462\n",
            "Epoch 21  loss=0.1436\n",
            "Epoch 22  loss=0.1431\n",
            "Epoch 23  loss=0.1412\n",
            "Epoch 24  loss=0.1405\n",
            "Epoch 25  loss=0.1400\n",
            "Epoch 26  loss=0.1370\n",
            "Epoch 27  loss=0.1373\n",
            "Epoch 28  loss=0.1370\n",
            "Epoch 29  loss=0.1351\n",
            "Epoch 30  loss=0.1336\n"
          ]
        }
      ],
      "source": [
        "model = SiameseMLP(len(digraph_cols), d_emb=EMBED_DIM, p_drop=DROPOUT_P).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    ds = PairDataset(train_df, digraph_cols, MAX_POS_PAIRS_PER_USER, NEGATIVES_PER_POSITIVE, seed=np.random.randint(1_000_000))\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "    model.train(); total, steps = 0.0, 0\n",
        "    for xa, xb, y in loader:\n",
        "        xa, xb, y = xa.to(DEVICE), xb.to(DEVICE), y.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        za, zb = model(xa, xb)\n",
        "        loss = contrastive_loss(za, zb, y, margin=MARGIN)\n",
        "        loss.backward(); opt.step()\n",
        "        total += loss.item(); steps += 1\n",
        "    print(f\"Epoch {ep:02d}  loss={total/max(1,steps):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7879ae14"
      },
      "source": [
        "## 7) Evaluate per-user binary metrics (macro means)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bd82f44",
        "outputId": "88a5d6bc-e9cf-410f-c51a-0847000ebec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user  Precision    Recall        F1       FAR       FRR       tau\n",
            "0  p001   1.000000  0.933333  0.965517  0.000000  0.033333  0.520032\n",
            "1  p002   0.877551  0.716667  0.788991  0.050000  0.141667  0.669773\n",
            "2  p003   0.935484  0.966667  0.950820  0.033333  0.016667  0.609647\n",
            "3  p004   0.812500  0.866667  0.838710  0.100000  0.066667  0.696776\n",
            "4  p005   0.702381  0.983333  0.819444  0.208333  0.008333  0.902098\n",
            "\n",
            "=== Macro means over users ===\n",
            "Precision: 0.8464\n",
            "Recall: 0.9248\n",
            "F1: 0.8786\n",
            "FAR: 0.0944\n",
            "FRR: 0.0376\n"
          ]
        }
      ],
      "source": [
        "def evaluate_per_user_binary(model, train_df, test_df, feature_cols):\n",
        "    model.eval(); rng = np.random.RandomState(SEED+7)\n",
        "\n",
        "    # embed test samples for each user once\n",
        "    test_embeds = {}\n",
        "    with torch.no_grad():\n",
        "        for uid, sub in test_df.groupby('participant'):\n",
        "            X = sub[feature_cols].values.astype('float32')\n",
        "            if len(X)==0: continue\n",
        "            test_embeds[uid] = model.embed(torch.from_numpy(X).to(DEVICE)).cpu().numpy()\n",
        "\n",
        "    rows = []\n",
        "    with torch.no_grad():\n",
        "        for uid, tr in train_df.groupby('participant'):\n",
        "            X_enroll = tr[feature_cols].values.astype('float32')\n",
        "            if len(X_enroll) < 2: continue\n",
        "            Z_enroll = model.embed(torch.from_numpy(X_enroll).to(DEVICE)).cpu().numpy()\n",
        "            template = Z_enroll.mean(axis=0, keepdims=True)\n",
        "            d_enroll = np.linalg.norm(Z_enroll - template, axis=1)\n",
        "            tau = float(np.clip(d_enroll.mean() + 3.0*d_enroll.std(), 0.0, 2.0))\n",
        "\n",
        "            Z_pos = test_embeds.get(uid)\n",
        "            if Z_pos is None or len(Z_pos)==0: continue\n",
        "            d_pos = np.linalg.norm(Z_pos - template, axis=1)\n",
        "\n",
        "            others = [Z for u2, Z in test_embeds.items() if u2 != uid and len(Z)>0]\n",
        "            if not others: continue\n",
        "            pool = np.concatenate(others, axis=0)\n",
        "            sel = rng.choice(len(pool), size=len(d_pos), replace=len(pool)<len(d_pos))\n",
        "            Z_neg = pool[sel]\n",
        "            d_neg = np.linalg.norm(Z_neg - template, axis=1)\n",
        "\n",
        "            y_true = np.r_[np.ones_like(d_pos), np.zeros_like(d_neg)]\n",
        "            y_pred = np.r_[(d_pos <= tau).astype(int), (d_neg <= tau).astype(int)]\n",
        "\n",
        "            P, R, F1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "            FAR = ((y_pred==1)&(y_true==0)).mean()\n",
        "            FRR = ((y_pred==0)&(y_true==1)).mean()\n",
        "            rows.append((uid, P, R, F1, FAR, FRR, tau))\n",
        "\n",
        "    out = pd.DataFrame(rows, columns=['user','Precision','Recall','F1','FAR','FRR','tau'])\n",
        "    macro = out[['Precision','Recall','F1','FAR','FRR']].mean().to_dict()\n",
        "    return out, macro\n",
        "\n",
        "per_user, macro = evaluate_per_user_binary(model, train_df, test_df, digraph_cols)\n",
        "print(per_user.head())\n",
        "print(\"\\n=== Macro means over users ===\")\n",
        "for k,v in macro.items(): print(f\"{k}: {v:.4f}\")"
      ]
    }
  ]
}